## :watermelon: 深度学习笔记[^1]

### [安装](./Week01/Day1.ipynb)

### 1. [引言](./Week01/Day2.ipynb)

### 2. 预备知识

- 2.1 [数据操作](./Week01/Day3.ipynb)
- 2.2 [数据预处理](./Week01/Day4.ipynb)
- 2.3 [线性代数](./Week01/Day5.ipynb)
- 2.4 [微积分](./Week01/Day6.ipynb)
- 2.5 [自动微分](./Week01/Day7.ipynb)
- 2.6 [概率](./Week02/Day1.ipynb)
- 2.7 查阅文档

### 3. 线性神经网络
- 3.1 线性回归
- 3.2 线性回归的从零开始实现
- 3.3 线性回归的简洁实现
- 3.4 softmax回归
- 3.5 图像分类数据集
- 3.6 softmax回归的从零开始实现
- 3.7 softmax回归的简洁实现

### 4. 多层感知机
- 4.1 多层感知机
- 4.2 多层感知机的从零开始实现
- 4.3 多层感知机的简洁实现
- 4.4 模型选择、欠拟合和过拟合
- 4.5 权重衰减
- 4.6 暂退法（Dropout）
- 4.7 前向传播、反向传播和计算图
- 4.8 数值稳定性和模型初始化
- 4.9 环境和分布偏移
- 4.10 实战Kaggle比赛：预测房价

### 5. 深度学习计算
- 5.1 层和块
- 5.2 参数管理
- 5.3 延后初始化
- 5.4 自定义层
- 5.5 读写文件
- 5.6 GPU

### 6. 卷积神经网络
- 6.1 从全连接层到卷积
- 6.2 图像卷积
- 6.3 填充和步幅
- 6.4 多输入多输出通道
- 6.5 汇聚层
- 6.6 卷积神经网络（LeNet）
- 6.7 深度卷积神经网络（AlexNet）
- 6.8 使用块的网络（VGG）
- 6.9 NiN 与 1x1 卷积层
- 6.10 含并行连结的网络（GoogLeNet）

### 7. 循环神经网络
- 7.1 循环神经网络
- 7.2 语言模型和数据集
- 7.3 循环神经网络的从零开始实现
- 7.4 循环神经网络的简洁实现
- 7.5 通过时间反向传播
- 7.6 门控循环单元（GRU）
- 7.7 长短期记忆网络（LSTM）

### 8. 注意力机制
- 8.1 注意力提示
- 8.2 注意力汇聚：Nadaraya-Watson 核回归
- 8.3 多注意力头机制
- 8.4 自注意力和位置编码
- 8.5 Transformer

### 9. 生成对抗网络
- 9.1 生成对抗网络简介
- 9.2 深度卷积生成对抗网络（DCGAN）
- 9.3 条件生成对抗网络（cGAN）

### 10. 自然语言处理应用
- 10.1 词嵌入
- 10.2 预训练词向量
- 10.3 编码器-解码器
- 10.4 注意力机制在序列到序列模型中的应用
- 10.5 BERT
- 10.6 GPT

### 11. 计算机视觉应用
- 11.1 目标检测和边界框
- 11.2 锚框
- 11.3 单发多框检测（SSD）
- 11.4 区域建议网络（RPN）
- 11.5 图像分割
- 11.6 实例分割

### 12. 强化学习
- 12.1 智能体和环境
- 12.2 马尔可夫决策过程
- 12.3 动态规划
- 12.4 无模型强化学习：策略评估
- 12.5 无模型强化学习：控制
- 12.6 深度Q网络
- 12.7 策略梯度 

[^1]: 教程来源: [动手学深度学习](https://zh.d2l.ai/)